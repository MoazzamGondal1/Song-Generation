{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  \n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "  # Remove all other punctuation\n",
    "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "  # Make it lowercase\n",
    "  dataset[field] = dataset[field].str.lower()\n",
    "  # Make it one long string to split by line\n",
    "  lyrics = dataset[field].str.cat()\n",
    "  corpus = lyrics.split('\\n')\n",
    "  # Remove any trailing whitespace\n",
    "  for l in range(len(corpus)):\n",
    "    corpus[l] = corpus[l].rstrip()\n",
    "  # Remove any empty lines\n",
    "  corpus = [l for l in corpus if l != '']\n",
    "\n",
    "  return corpus\n",
    "\n",
    "def get_time():\n",
    "  # Get the current date and time\n",
    "  current_datetime = datetime.now()\n",
    "\n",
    "  formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H%M\")\n",
    "\n",
    "  return formatted_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting 100 hip hop songs randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Artist\n",
       "Travis-Scott      28\n",
       "Kendrick-Lamar    21\n",
       "Eminem            17\n",
       "J-Cole            16\n",
       "Snoop-Dogg        16\n",
       "Maroon-5           1\n",
       "Ed-Sheeran         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_songs = pd.read_csv(\"SongsData_hiphop.csv\")\n",
    "random_songs_df = all_songs.sample(n=100, random_state=42)\n",
    "random_songs_df['Artist'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokeniziation and BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2934\n"
     ]
    }
   ],
   "source": [
    "corpus = create_lyrics_corpus(random_songs_df,\"Lyrics\")\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences to fit in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tsequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_pad_len = max([len(seq) for seq in sequences])\n",
    "print(max_pad_len)\n",
    "padded_sequences = np.array(pad_sequences(sequences,maxlen=max_pad_len,truncating='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences, labels = padded_sequences[:,:-1], padded_sequences[:,-1]\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "486/486 [==============================] - 104s 200ms/step - loss: 6.7621 - accuracy: 0.0387\n",
      "Epoch 2/10\n",
      "486/486 [==============================] - 103s 211ms/step - loss: 7.0289 - accuracy: 0.0353\n",
      "Epoch 3/10\n",
      "486/486 [==============================] - 97s 199ms/step - loss: 7.2115 - accuracy: 0.0388\n",
      "Epoch 4/10\n",
      "486/486 [==============================] - 95s 196ms/step - loss: 7.0415 - accuracy: 0.0482\n",
      "Epoch 5/10\n",
      "486/486 [==============================] - 93s 191ms/step - loss: 6.8568 - accuracy: 0.0526\n",
      "Epoch 6/10\n",
      "486/486 [==============================] - 91s 187ms/step - loss: 6.6137 - accuracy: 0.0597\n",
      "Epoch 7/10\n",
      "486/486 [==============================] - 91s 188ms/step - loss: 6.4451 - accuracy: 0.0693\n",
      "Epoch 8/10\n",
      "486/486 [==============================] - 92s 190ms/step - loss: 6.3657 - accuracy: 0.0766\n",
      "Epoch 9/10\n",
      "486/486 [==============================] - 91s 188ms/step - loss: 6.1121 - accuracy: 0.0827\n",
      "Epoch 10/10\n",
      "486/486 [==============================] - 91s 188ms/step - loss: 5.9127 - accuracy: 0.0903\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 200, input_length=max_pad_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_model = load_model(\"hiphop_model_2023-12-03_0352.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "486/486 [==============================] - 101s 200ms/step - loss: 5.3188 - accuracy: 0.1181\n",
      "Epoch 2/2\n",
      "486/486 [==============================] - 104s 214ms/step - loss: 5.3452 - accuracy: 0.1250\n"
     ]
    }
   ],
   "source": [
    "history = reload_model.fit(input_sequences, one_hot_labels, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiphop_model_2023-12-03_0357.h5\n"
     ]
    }
   ],
   "source": [
    "model_name = \"hiphop_model_\" + get_time() + \".h5\"\n",
    "reload_model.save(model_name)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "there will be blood slick thousand 'til hours you cowards hours handled life how swag swag swag flight under roll both dusk swag and worse provide function i feel caved it cole boobs boobs\n"
     ]
    }
   ],
   "source": [
    "# Use this process for the full output generation\n",
    "seed_text = \"there will be blood\"\n",
    "next_words = 30\n",
    "  \n",
    "for _ in range(next_words):\n",
    "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "  token_list = pad_sequences([token_list], maxlen=max_pad_len-1, padding='post')\n",
    "  predicted_probs = reload_model.predict(token_list)[0]\n",
    "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
    "                               p=predicted_probs)\n",
    "  output_word = \"\"\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == predicted:\n",
    "      output_word = word\n",
    "      break\n",
    "  seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
